# ğŸ“¦KubetorchğŸ”¥

**A Fast, Pythonic, "Serverless" Interface for Running ML Workloads on Kubernetes**

Kubetorch lets you programmatically build, iterate, and deploy ML applications on Kubernetes at any scale - directly from Python.

It brings your cluster's compute power into your local development environment, enabling extremely fast iteration (1-2 seconds). Logs, exceptions, and hardware faults are automatically propagated back to you in real-time.

Since Kubetorch has no local runtime or code serialization, you can access large-scale cluster compute from any Python environment - your IDE, notebooks, CI pipelines, or production code - just like you would use a local process pool.

## What Kubetorch Enables

- **100x faster iteration** from 10+ minutes to 1-3 seconds for complex ML applications like RL and distributed training
- **50%+ compute cost savings** through intelligent resource allocation, bin-packing, and dynamic scaling
- **95% fewer production faults** with built-in fault handling with programmatic error recovery and resource adjustment

---

[Apache 2.0 License](https://github.com/run-house/runhouse/blob/main/LICENSE)

**ğŸƒâ€â™€ï¸ Built by [Runhouse](https://www.run.house) ğŸ **
