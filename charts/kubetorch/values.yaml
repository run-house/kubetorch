kubetorchConfig:
  serviceAccountAnnotations: {}
  # List of namespaces to allow deploying compute resources to
  deployment_namespaces:
    - "default"
  # Enable live log streaming
  logStreamingEnabled: true
  # Enable live metrics streaming (required for TTL/auto-scaling)
  metricsEnabled: true

kubetorchController:
  image: ghcr.io/run-house/kubetorch-controller
  tag: 0.4.1
  imagePullPolicy: Always

  servicePort: 8080  # Port the controller service is exposed on (via nginx)
  port: 8081  # Internal port the controller listens on
  connectionPoolSize: 20  # K8s API connection pool size per worker
  storage:
    size: "1Gi"  # Size of the PVC for controller state
    # storageClassName: ""  # Optional: specify a storage class (defaults to cluster default)

  # Event watcher configuration (streams K8s events to Loki for client-side display)
  eventWatcher:
    enabled: true  # flip to `false` to disable K8s event watching
    batchSize: 10  # Number of events to batch before pushing to Loki
    flushInterval: 1.0  # Max seconds between flushes (lower = faster delivery)

  resources:
    cpu:
      request: "1"   # Minimum guaranteed CPU
    memory:
      request: "2Gi"    # Minimum guaranteed memory

  tolerations: []
  affinity: {}

  # TTL configuration
  ttl:
    enabled: true
    # Interval in seconds between TTL checks (runs as background task in controller)
    intervalSeconds: 300
    # Prometheus URL for the TTL Controller to query metrics
    prometheusUrl: "http://kubetorch-metrics.kubetorch.svc.cluster.local:9090"
    # PodMonitor for scraping deployed service pod metrics (for kube-prometheus-stack users)
    # kubetorch-metrics already scrapes these pods, so disabled by default
    podMonitor:
      enabled: false
      additionalLabels: {}
      prometheusLabel: "kube-prometheus" # Label to match your Prometheus Operator instance

  # Nginx sidecar configuration (runs alongside controller in same pod)
  nginx:
    resolver: "kube-dns.kube-system.svc.cluster.local"  # DNS resolver for upstream resolution
    image:
      repository: nginx
      tag: 1.29.0-alpine
      pullPolicy: IfNotPresent
    maxBodySize:
      rsync: "10G"   # Max body size for rsync operations (large data transfers)
      api: "250M"    # Max body size for API routes
    healthRoute: "/health"
    resources:
      cpu:
        request: "200m"
      memory:
        request: "256Mi"

nvidia-device-plugin:
  enabled: true # set to `false` to disable GPU support
  namespaceOverride: "kubetorch"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: Exists

  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "dedicated"
      operator: "Equal"
      value: "gpu"
      effect: "NoSchedule"

dataStore:
  image: ghcr.io/run-house/kubetorch-data-store:0.4.1
  imagePullPolicy: Always
  # serviceAccount: kubetorch-service-account  # Optional: use specific service account for Workload Identity
  metadataPort: 8081  # Port for metadata server API
  maxConnections: 500  # Maximum concurrent rsync connections (increase for many worker pods)
  timeout: 600  # Connection timeout in seconds
  maxVerbosity: 0  # Log verbosity (0-4, use 0 for production, higher for debugging)
  maxConnectionsPerModule: 0  # Per-module limit (0 = unlimited, inherits global limit)
  cpu:
    request: 1
  memory:
    request: 4Gi
  ephemeralStorage: # adjust based on expected node disk size
    request: 5Gi
  storage:
    size: 100Gi  # Size of persistent volume
    storageClassName: ""  # Leave empty to use default storage class
  cleanupCron:
    enabled: false # set to true to enable pod cleanup
  tolerations: []
  affinity: {}

logStreaming:
  enabled: true # set to `false` to disable live log streaming
  retentionPeriod: 24h
  maxConcurrentTailRequests: 100
  maxGlobalStreamsPerUser: 50000
  ingestionRateMb: 10
  ingestionBurstSizeMb: 20
  port: 3100

metrics:
  enabled: true # set to `false` to disable live metrics streaming
  scrapeKubelet: true
  prometheus:
    image: prom/prometheus:v3.7.2
    port: 9090
    retention: 24h
    scrapeInterval: 3s
    resources:
      cpu: 200m
      memory: 512Mi
    # Remote write configuration (e.g., for Grafana Cloud)
    remoteWrite:
      enabled: false
      url: ""  # e.g., https://prometheus-prod-XX-prod-us-central-0.grafana.net/api/prom/push
      # Name of existing secret with 'username' and 'password' keys
      basicAuthSecret: ""  # e.g., grafana-cloud-credentials
      insecureSkipVerify: true  # Set to false for strict TLS verification
    # DCGM exporter scrape config (auto-discovers on GKE, EKS, AKS)
    additionalScrapeConfigs:
      - job_name: "dcgm-exporter"
        honor_labels: true
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ["gke-managed-system", "gpu-operator", "nvidia-gpu-operator"]
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_name]
            regex: ".*dcgm-exporter.*"
            action: keep
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            regex: "9400"
            action: keep
          - source_labels: [__meta_kubernetes_pod_ip]
            target_label: __address__
            replacement: "$1:9400"
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

dcgm-exporter:
  enabled: false  # set to true to deploy DCGM exporter with this chart
  namespaceOverride: "kubetorch"
  serviceMonitor:
    enabled: false  # kubetorch deploys its own service monitoring deployment

  image:
    repository: nvcr.io/nvidia/k8s/dcgm-exporter
    tag: 4.4.1-4.6.0-ubuntu22.04

  readinessProbe:
    enabled: false

  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          # AWS GPUs
          - matchExpressions:
              - key: karpenter.k8s.aws/instance-gpu-manufacturer
                operator: In
                values: ["nvidia"]
              - key: karpenter.k8s.aws/instance-gpu-name
                operator: In
                values: ["a10g", "a100", "t4"]
          # Fallback: any node with a GPU
          - matchExpressions:
              - key: nvidia.com/gpu.present
                operator: Exists
