import datetime
import hashlib
import json
import re
import subprocess

from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from runhouse.constants import (
    CLUSTER_CONFIG_PATH,
    EMPTY_DEFAULT_ENV_NAME,
    LAST_ACTIVE_AT_TIMEFRAME,
    RESERVED_SYSTEM_NAMES,
    TIME_UNITS,
)
from runhouse.globals import rns_client

from runhouse.logger import get_logger
from runhouse.resources.envs.utils import _get_env_from, run_setup_command
from runhouse.resources.hardware.sky.command_runner import (
    _HASH_MAX_LENGTH,
    ssh_options_list,
    SshMode,
)

logger = get_logger(__name__)


class ServerConnectionType(str, Enum):
    """Manage the type of connection Runhouse will make with the API server started on the cluster.
    ``ssh``: Use port forwarding to connect to the server via SSH, by default on port 32300.
    ``tls``: Do not use port forwarding and start the server with HTTPS (using custom or fresh TLS certs), by default
        on port 443.
    ``none``: Do not use port forwarding, and start the server with HTTP, by default on port 80.
    """

    SSH = "ssh"
    TLS = "tls"
    NONE = "none"


class ResourceServerStatus(str, Enum):
    running = "running"
    terminated = "terminated"
    unauthorized = "unauthorized"
    unknown = "unknown"
    internal_server_error = "internal_server_error"
    runhouse_daemon_down = "runhouse_daemon_down"
    invalid_url = "invalid_url"
    local_cluster = "local_cluster"


class ClustersListStatus(str, Enum):
    running = "running"
    terminated = "terminated"
    down = "down"


def cluster_config_file_exists() -> bool:
    return Path(CLUSTER_CONFIG_PATH).expanduser().exists()


def load_cluster_config_from_file() -> Dict:
    if cluster_config_file_exists():
        with open(Path(CLUSTER_CONFIG_PATH).expanduser()) as f:
            cluster_config = json.load(f)
        return cluster_config
    else:
        return {}


def _current_cluster(key="config"):
    """Retrieve key value from the current cluster config.
    If key is "config", returns entire config."""
    from runhouse.globals import obj_store

    cluster_config = obj_store.get_cluster_config()
    cluster_config.pop("creds", None)
    if cluster_config:
        # This could be a local cluster started via runhouse start,
        # in which case it would have no Name.
        if key in ["cluster_name", "name"] and "name" not in cluster_config:
            return None
        if key == "config":
            return cluster_config
        if key == "cluster_name":
            return cluster_config["name"].rsplit("/", 1)[-1]
        return cluster_config[key]
    else:
        return None


def _default_env_if_on_cluster():
    from runhouse import Env

    config = _current_cluster()
    return (
        _get_env_from(
            config.get(
                "default_env",
                Env(name=EMPTY_DEFAULT_ENV_NAME),
            )
        )
        if config
        else None
    )


def _get_cluster_from(system, dryrun=False):
    from .cluster import Cluster

    if isinstance(system, Cluster):
        return system
    if system in RESERVED_SYSTEM_NAMES:
        return system

    if isinstance(system, Dict):
        return Cluster.from_config(system, dryrun)

    if isinstance(system, str):
        config = _current_cluster(key="config")
        if config and system == config.get("name"):
            return Cluster.from_config(config, dryrun)
        try:
            system = Cluster.from_name(name=system, dryrun=dryrun)
        except ValueError:
            # Name not found in RNS. Doing the lookup this way saves us a hop to RNS
            pass

    return system


def _unnamed_default_env_name(cluster_name):
    return f"{cluster_name}_default_env"


def detect_cuda_version_or_cpu(cluster: "Cluster" = None, node: Optional[str] = None):
    """Return the CUDA version on the cluster. If we are on a CPU-only cluster return 'cpu'.

    Note: A cpu-only machine may have the CUDA toolkit installed, which means nvcc will still return
    a valid version. Also check if the NVIDIA driver is installed to confirm we are on a GPU."""

    status_codes = run_setup_command("nvcc --version", cluster=cluster, node=node)
    if not status_codes[0] == 0:
        return "cpu"
    cuda_version = status_codes[1].split("release ")[1].split(",")[0]

    if run_setup_command("nvidia-smi", cluster=cluster, node=node)[0] == 0:
        return cuda_version
    return "cpu"


def _run_ssh_command(
    address: str,
    ssh_user: str,
    ssh_port: int,
    ssh_private_key: str,
    ssh_proxy_command: str,
    docker_user: str,
):
    from runhouse.resources.hardware.sky_command_runner import SkySSHRunner

    runner = SkySSHRunner(
        (address, ssh_port),
        ssh_user=ssh_user,
        ssh_private_key=ssh_private_key,
        ssh_proxy_command=ssh_proxy_command,
        docker_user=docker_user,
    )
    ssh_command = runner._ssh_base_command(
        ssh_mode=SshMode.INTERACTIVE, port_forward=None
    )
    subprocess.run(ssh_command)


def _docker_ssh_proxy_command(
    address: str,
    ssh_user: str,
    ssh_private_key: str,
):
    return lambda ssh: " ".join(
        ssh
        + ssh_options_list(ssh_private_key, None)
        + ["-W", "%h:%p", f"{ssh_user}@{address}"]
    )


# Adapted from SkyPilot Command Runner
def _ssh_base_command(
    address: str,
    ssh_user: str,
    ssh_private_key: str,
    ssh_control_name: Optional[str] = "__default__",
    ssh_proxy_command: Optional[str] = None,
    ssh_port: int = 22,
    docker_ssh_proxy_command: Optional[str] = None,
    disable_control_master: Optional[bool] = False,
    ssh_mode: SshMode = SshMode.INTERACTIVE,
    port_forward: Optional[List[int]] = None,
    connect_timeout: Optional[int] = 30,
):
    ssh = ["ssh"]
    if ssh_mode == SshMode.NON_INTERACTIVE:
        # Disable pseudo-terminal allocation. Otherwise, the output of
        # ssh will be corrupted by the user's input.
        ssh += ["-T"]
    else:
        # Force pseudo-terminal allocation for interactive/login mode.
        ssh += ["-tt"]

    if port_forward is not None:
        # RH MODIFIED: Accept port int (to forward same port) or pair of ports
        for fwd in port_forward:
            if isinstance(fwd, int):
                local, remote = fwd, fwd
            else:
                local, remote = fwd
            logger.debug(f"Forwarding port {local} to port {remote} on localhost.")
            ssh += ["-L", f"{local}:localhost:{remote}"]

    return (
        ssh
        + ssh_options_list(
            ssh_private_key,
            ssh_control_name,
            ssh_proxy_command=ssh_proxy_command,
            docker_ssh_proxy_command=docker_ssh_proxy_command,
            # TODO change to None like before?
            port=ssh_port,
            connect_timeout=connect_timeout,
            disable_control_master=disable_control_master,
        )
        + [f"{ssh_user}@{address}"]
    )


def _generate_ssh_control_hash(ssh_control_name):
    return hashlib.md5(ssh_control_name.encode()).hexdigest()[:_HASH_MAX_LENGTH]


def up_cluster_helper(cluster, capture_output: Union[bool, str] = True):
    from runhouse.utils import SuppressStd

    if capture_output:
        try:
            with SuppressStd() as outfile:
                cluster.up()
        except Exception as e:
            if isinstance(capture_output, str):
                logger.error(
                    f"Error starting cluster {cluster.name}, logs written to {capture_output}"
                )
            raise e
        finally:
            if isinstance(capture_output, str):
                with open(capture_output, "w") as f:
                    f.write(outfile.output)
    else:
        cluster.up()


###################################
# Cluster list helping methods
###################################


def parse_time_duration(duration: str):
    # A simple parser for duration like "15m", "2h", "3d"
    try:
        unit = duration[-1]
        value = int(duration[:-1])

        time_filter_match = re.match(r"(\d+)([smhd])$", duration)

        if time_filter_match:

            # if the user provides a "--since" time filter than is less than a minute, set it by default to one minute.
            if unit == "s" and value < 60:
                value = 60

            time_duration = value * TIME_UNITS.get(unit)

        else:
            logger.warning(
                f"Filter is not applied, invalid time unit provided ({unit}). Setting filter to default (24h)."
            )
            time_duration = LAST_ACTIVE_AT_TIMEFRAME

        return time_duration
    except Exception:
        return LAST_ACTIVE_AT_TIMEFRAME


def parse_filters(since: str, cluster_status: str):
    cluster_filters = {}

    if since:
        last_active_in: int = parse_time_duration(
            duration=since
        )  # return in representing the "since" filter in seconds
        if last_active_in:
            cluster_filters["since"] = last_active_in

    if cluster_status:

        if cluster_status.lower() == ClustersListStatus.down:
            cluster_status = ResourceServerStatus.runhouse_daemon_down

        cluster_filters["cluster_status"] = cluster_status

    return cluster_filters


def get_clusters_from_den(cluster_filters: dict):
    get_clusters_params = {"resource_type": "cluster", "folder": rns_client.username}

    # send den request with filters if the user specifies filters.
    # If "all" filter is specified - get all clusters (no filters are added to get_clusters_params)
    if cluster_filters and "all" not in cluster_filters.keys():
        get_clusters_params.update(cluster_filters)

    # If not filters are specified, get only running clusters.
    elif not cluster_filters:
        get_clusters_params.update(
            {"cluster_status": "running", "since": LAST_ACTIVE_AT_TIMEFRAME}
        )

    clusters_in_den_resp = rns_client.session.get(
        f"{rns_client.api_server_url}/resource",
        params=get_clusters_params,
        headers=rns_client.request_headers(),
    )

    return clusters_in_den_resp


def get_unsaved_live_clusters(den_clusters: list[Dict]):
    import sky

    try:
        sky_live_clusters: list = sky.status()
        den_clusters_names = [c.get("name") for c in den_clusters]

        # getting the on-demand clusters that are not saved in den.
        if sky_live_clusters:
            return [
                cluster
                for cluster in sky_live_clusters
                if f'/{rns_client.username}/{cluster.get("name")}'
                not in den_clusters_names
            ]
        else:
            return []
    except Exception as e:
        logger.debug(f"Failed to get unsaved sky live clusters: {e}")
        return []


def get_all_sky_clusters():
    import sky

    try:
        sky_live_clusters: list = sky.status()

        # getting the on-demand clusters that are not saved in den.
        if sky_live_clusters:
            return [cluster.get("name") for cluster in sky_live_clusters]
        else:
            return []
    except Exception as e:
        logger.debug(f"Failed to get sky live clusters: {e}")
        return []


def cluster_last_active_from_datetime_to_str(clusters: List[Dict[str, Any]]):
    for cluster in clusters:
        cluster["Last Active (UTC)"] = cluster.get("Last Active (UTC)").strftime(
            "%m/%d/%Y, %H:%M:%S"
        )
    return clusters


def get_running_and_not_running_clusters(clusters: list):
    running_clusters, not_running_clusters = [], []

    for den_cluster in clusters:
        # get just name, not full rns address. reset is used so the name will be printed all in white.
        cluster_name = den_cluster.get("name").split("/")[-1]
        cluster_type = den_cluster.get("data").get("resource_subtype")
        cluster_status = (
            den_cluster.get("status") if den_cluster.get("status") else "unknown"
        )

        # currently relying on status pings to den as a sign of cluster activity.
        # The split is required to remove milliseconds and the offset (according to UTC) from the timestamp.
        # (status_last_checked is in the following format: YYYY-MM-DD HH:MM:SS.ssssss±HH:MM)

        last_active_at = den_cluster.get("status_last_checked")
        last_active_at = (
            datetime.datetime.fromisoformat(last_active_at.split(".")[0])
            if isinstance(last_active_at, str)
            else datetime.datetime(1970, 1, 1)
        )  # Convert to datetime
        last_active_at = last_active_at.replace(tzinfo=datetime.timezone.utc)

        if cluster_status == "running" and not last_active_at:
            # For BC, in case there are clusters that were saved and created before we introduced sending cluster status to den.
            cluster_status = "unknown"

        cluster_info = {
            "Name": cluster_name,
            "Cluster Type": cluster_type,
            "Status": cluster_status,
            "Last Active (UTC)": last_active_at,
        }
        running_clusters.append(
            cluster_info
        ) if cluster_status == "running" else not_running_clusters.append(cluster_info)

    # Sort clusters by the 'Last Active (UTC)' and 'Status' column
    not_running_clusters = sorted(
        not_running_clusters,
        key=lambda x: (x["Last Active (UTC)"], x["Status"]),
        reverse=True,
    )

    not_running_clusters = cluster_last_active_from_datetime_to_str(
        clusters=not_running_clusters
    )

    running_clusters = sorted(
        running_clusters, key=lambda x: x["Last Active (UTC)"], reverse=True
    )

    running_clusters = cluster_last_active_from_datetime_to_str(
        clusters=running_clusters
    )

    return running_clusters, not_running_clusters


def get_saved_logs_from_den(rns_address: str):
    """
    get the latest cluster logs saved in den.
    """
    cluster_uri = rns_client.resource_uri(rns_address)
    clusters_in_den_resp = rns_client.session.get(
        f"{rns_client.api_server_url}/resource/{cluster_uri}/logs",
        headers=rns_client.request_headers(),
    )
    return clusters_in_den_resp
